---
layout: post
title:  From Recognition to Cognition Visual Commonsense Reasoning 
subtitle: Commonsense Reasoning
categories: Paper Review
---

<!-- ![_config.yml]({{ site.baseurl }}./images/logo.png) -->

"CVPR 2019, Commonsense Reasoning 리뷰 "

### 1. 서론

사람은 이미지 한 장면만 봐도 그 장면 너머의 벌어졌던 일들에 대해서 추론이 가능하다. 
![]({{site.url}}/images/20210112/image1.png)

이 사진을 보면 사람들, 접시들, 컵 뿐만 아니라 전반적인 상황을 알 수 있다. 예를 들어 이 한 장면을 보고사람들이 “저녁을 같이 먹고 있고”, “이미 주문을 시켰고”, “사람3은 먹는 사람이 아니라 주문하는 사람이고”, “사람1이 베이컨이랑 팬케잌을 주문했다는 것.”을 사람은 인지할 수 있다.
현재의 최신 비전 시스템들은 오브젝트 디텍션, 세그멘테이션과 같은 지각 수준의 이미지 이해는 잘 하고 있지만 앞선 장면의 예시와 같이, 조금 더 복잡한 단계의 추론에 대해서는 어려움이 있다. 따라서 이 논문에서는 시각 이해 단계를 위해 Visual Commonsense Reasoning이라는 태스크를 제시한다.

이미지가 주어지면 기계는 이미지로부터 비롯되는 시각 세계의 이해를 필요로 하는 질문에 대해 대답한다. 무엇보다도 기계는 정답인 응답에 대해서, 장면의 세부사항과 배경지식(background knowledge)를 이용하여 정답인지 옳다고 주장하는(justifying) 이유(rationale)와 의도(intentions)를 선택할 수 있어야한다.
이러한 질문들과 응답들 그리고 이유들은 풍부한 자연 언어와 이미지 영역들에 대한 참조로 이루어져 있다. 명확한 평가를 위해 모든 태스크들은 multiple choice QA로 구성하였다.
이 태스크를 위한 새로운 데이터셋인 VCR은 290k의 대규모 질문, 응답, 이유들로 이루어져있으며 110k 개의 독립된 영화 장면들로 이루어져 있다. 이와 같은 규모의 데이터셋을 구성할 때 가장 중요한 문제점은 어떻게 하면 인공적인 주석을 피할 수 있을 것인지에 대한 점이다. 
최근의 질의 응답 데이터셋들은 의도하지 않았지만 정답을 쓰는 과정에서 사람에 의한 의도치 않은 편향이 정답에 포함되고 모델은 이를 손쉽게 찾아낸다. 이런 편향들은 너무 유력해서 모델이 때때로 문제를 보지도 않고 정답을 골라낼 수 있다. 따라서 우리는 강건한 multiple choice 데이터셋을 만들기 위한 새로운 QA 할당 알고리즘인 Adversarial Matching를 제시한다. 주요한 아이디어는 질문마다 있는 정답을 다른 세 질문들에는 오답으로 재활용하는 것이다. 이런 접근법은 정답에만 보통 포함되는 편향(answer-only biases)를 해결할 수 있고 기계가 일반적으로 정답이라고 판단하는 선택과정에서 어렵게 만들 수 있다. 우리는 이와 같은 정답 재활용 과정을 거칠 때, 최신 성능의 언어 모델에 의해 측정된 실제 정답과 후보인 가짜 정답들의 관련성(relevance)과 수반성(entailment score)를 활용하여 구성하였다. 우리의 재활용 알고리즘이 사람과 기계의 어려움 사이의 트레이드오프를 조절하는 키가 되었다. 우리는 사람에겐 쉽고 기계는 어려운 문제를 만들고 싶었기 때문이다.
이미지를 이해하는데 지각(recognition) 수준과 인지(cognition) 수준의 차이를 좁히는 것은 시각 데이터 안의 자연어로 이루어진 짧은 글자에 대한 그라운딩(grounding)이 필요하고, 질문 내용 안에 있는 정답에 대한 이해와 질문과 응답 이유 그리고 이미지 전반이 공유(shared)되고 이해(grounded)된 추론 과정이 필요하다. 

이 논문에서는 이에 Recognition to Cognition Networks (R2C)라는 새로운 모델을 소개한다. 우리의 모델은 세가지 추론 과정을 거친다. 첫 번째로 이미지 영역(오브젝트)의 관점에서 자연 언어의 의미를 그라운드(ground)하고 두 번째로는 질문 및 언급된 적 없는 전반적인 오브젝트들의 관점에서 응답들의 의미를 문맥화(contextualization) 한다. 마지막으로 정답으로 가기 위한 공유된 표현을 전반적으로 추론(reason over)한다. VCR 데이터셋에 실험하였을 때, R2C 모델은 시각 질의 응답 시스템에서 최신 성능을 보여주었는데, 질의 응답(question answering)에서 65%, 정답 확정(answer justification)에서 67% 그리고 정답을 맞추고 정답을 확정하는데 44%의 성능을 보여주었다. 
여전히 태스크와 데이터셋은 문제를 푼 것과는 거리가 있지만 (사람은 90% 이상의 점수를 냈으므로) 우리는 미래의 연구로 향하기 위한 길목에서 자세한 인사이트를 제공한다.
요약하면 우리의 주요한 기여점은 4가지로 정리 될 수 있다. (1) 우리는 Visual Commonsense Reasoning이라는 새로운 태스크를 구성하였다. 그리고 (2) 큰 규모(large-scale)의 multiple-choice 질의응답 VCR 데이터셋을 제시한다. (3) 동시에 이를 자동으로 작성하고 강건한 multiple choice 질의 응답 데이터셋을 만드는데 필요한 새로운 알고리즘인 Adversarial Matching을 제안한다. (4) 우리는 동시에 지각 수준에서 인지 수준의 추론을 흉내 내는 새로운 모델인 R2C를 제안한다. 이 모델은 동시에 우리의 새로운 태스크의 베이스라인이다. 데이터셋과 코드는 다운로드하여 이용 가능하다.

### 2. 태스크 개요

VCR은 이미지에 대한 전반적이고 인지적인 이해가 필요하기 때문에 시각 시스템에서는 도전적인 새로운 태스크이다. 예를 들어서 사람3은 음식을 배달 중이며 사람1의 역할이 손님이라는 ‘역할에 대한 이해’나 사람1이 음식을 먹길 바라는 상태라는 ‘정신 상태(mental state)에 대한 이해’ 그리고 사람3이 팬케잌을 전달할 것이라는 장면의 이전 이후에 대한 “그럴듯한 사건”에 대한 이해가 필요하다. 우리 태스크는 이외에도 다른 카테고리들을 커버한다. 추론과 관련된 분포는 그림2에 정리하였다.
![]({{site.url}}/images/20210112/image2.png)
그림2: 약 38%정도의 질문들이 설명과 관련된 ‘why’나 ‘how’ 질문들이다. 약 24%가 인지(cognition-level) 수준의 행동들과 관련 있고 13%가 시간 추론 (temporal reasoning)과 관련 있다. 이런 카테고리는 서로 배타적이지 않고 대답을 하기 위해서는 여러 단계의 (several hop) 서로 다른 추론이 필요할 수도 있다. 

시각 이해는 질의 응답을 할 때 필요할 뿐만 아니라 제대로 된 이유를 응답하기 위해서도 필요하다. 우리는 따라서 모델이 왜 정답을 선택하였는지 이유(rationale)에 대해서 생성 (give)하는 것을 필요로 한다. 
우리의 질의와 응답과 이유들은 풍부한 자연 언어로 표현되어 있으며 “person 2”와 같은 오브젝트 태그(detection tag)로 이루어져있다. 이러한 태그들은 언어 설명(text description)과 이미지 영역의 모호하지 않은 연결을 돕는다.
평가를 진행하기 위해서 우리는 근본적인 태스크를 구성(frame) 하였다. Multiple-choice 환경에서 ‘answering’ 단계와 ‘justification’ 단이다. 하나의 질문에 4개의 응답 보기가 주어지고 모델은 옳은 정답을 먼저 선택해야만 한다. 그 다음 응답이 옳다면, 4개의 이유가 주어지고 (기계가 맞는 정답을 옳다고 주장(justify) 할 수 있도록) 옳은 이유를 고를 수 있어야한다. 우리는 이 과정을 Q->AR 이라고 부르고 선택된 정답과 선택된 이유가 모두 맞았을 때 모델 예측이 옳다고 판단한다.
우리의 태스크는 2개의 multi-choice 세부 태스크로 나뉠 수 있다. 각각 Q->A(answering) 그리고 QA->R(justification) 이다. 

```
정의 VCR 세부 태스크
VCR의 세부 태스크의 하나의 예제는 하나의 이미지 I 그리고
-	오브젝트 디텍션의 연속된 o, 각 오브젝트o_i는 bounding box b, 세그멘테이션 마스크 m 그리고 클래스 라벨 l_i로 이루어져 있다.
-	쿼리 q는 각 쿼리의 q_i는 단어장 V안의 한 단어 또는 태그이며 태그는 o의 오브젝트를 참조한다.
-	응답들의 집합 N, 각 응답 r^(i)은 질문과 유사하게 구성되어 있다. 정확하게 하나의 응답만 옳다.
모델은 최선인 하나의 응답을 고른다.  
```

Question-answering에선 쿼리는 질문이며 응답들은 정답 후보들이다.  Answer justification 에서는 쿼리는 질문 그리고 옳은 정답과 합쳐(concatenated)진다. 그리고 이유 선택지(rationale choices)들이 응답들이 된다.
이 논문에서는 우리는 모델을 4개의 응답들에 대한 정확도 측면에서 평가한다. 기본(baseline) 정확도는 각 서브 태스크마다 25% (1/N) 이다. 전체적으로 Q->AR은 기본 정확도가 6.25% (1/N^2) 가 되는 것이다. 

### 3. 데이터셋 수집

![]({{site.url}}/images/20210112/image7.png)

### 4. Adversarial Matching

![]({{site.url}}/images/20210112/image8.png)

### 5. Recognition to Cognition Networks

![]({{site.url}}/images/20210112/image9.png)

우리는 Recognition to Cognition Networks (R2C)를 소개한다. 시각 상식 추론을 하기 위한 새로운 모델이다. 
이 태스크를 잘 하기 위해서 언어, 시각, 그리고 세계에 대한 이해를 필요로 한다. 예를 들어 그림 5에서 ‘왜 사람4는 사람1을 가리키나?’를 추론하기 위해서는 여러 추론 단계가 필요하다. 먼저 우리는 각 응답에 대해서 쿼리의 의미를 이해하고 있어야 한다. 물론 이미지에서 두 사람에 대해 참조를 하고 있다는 것을 포함해서 말이다. 두 번째로, 우리는 쿼리, 응답 그리고 이미지의 의미를 문맥화 한다. 이 단계는 지시 대상인 ‘그’를 결정(resolve)하고 왜 저녁을 가리킬 것인지 결정하는 것을 포함한다.  
세 번째로 우리는 관련된 이미지의 영역들, 쿼리, 그리고 응답에 대한 상호 작용을 추론 해야한다. 이 예제로 예를 들면, 모델은 사람1과 사람4의 사회적인 상호 작용에 대해서 결정해야만 한다. 
우리는 모델을 세 단계로 구성 (formulate) 한다. Grounding, contextualization, and reasoning. 그리고 일반적인 뉴럴 모델 블록들을 이용해서 각 구성요소들을 구현한다. 
좀 더 자세히 말해서, 모델에 이미지 하나와 오브젝트 집합 o, 쿼리 하나 q 그리고 응답들의 집합 r이 주어진다. 쿼리 q 와 응답 r은 모두 자연 언어들로 표현되고 이미지의 영역을 참조한다. 우리는 오브젝트 태그들을 단어로 표현 (o_w) 할 것이다. 만약 w가 디텍션 태그가 아니라면 이미지 바운더리 전체를 참조한다. 우리의 모델은 아래 세 단계로 응답들을 각각 고려한다.

*Grounding* 그라운딩 모듈은 시퀀스의 각 토큰 마다의 이미지-언어 표현(representation) 동시에 배운다. 왜냐하면 쿼리와 응답들 모두 자연어와 오브젝트 태그들로 이루어져 있기 때문에 같은 그라운딩 모듈을 적용한다. (파라미터를 공유하도록 허락함) 그라운딩 모듈의 핵심은 양방향(bi-directional) LSTM이다. 
오브젝트 수준의 피처들을 배우기 위해서 CNN을 사용한다: 각 영역 o에 대한 시각 표현은 CNN의 bounding box 영역의 Roi-Aligned을 이용한다. 추가적으로 오브젝트의 클래스 라벨 l_o를 부호화(encode)하기 위해서 l_o의 임베딩을 공유하는 hidden representation으로 project한다. LSTM의 출력값을 쿼리의 q, 응답의 r의 모든 포지션이라고 하자.

*Contextualization*
쿼리와 응답에 대한 그라운드 된 표현이 주어지면, 어텐션 메커니즘을 사용해서 이 문장들을 각자의 관점에서 문맥화하고 이미지 문맥의 관점에서 문맥화를 진행한다. 각 위치 i 마다, 집중된 쿼리 표현을 q ̂_i 라고 정의하며 공식은 다음과 같다. 


### 부록 (Appendix)
 A.1 Language complexity and diversity
VCR 데이터셋에 있는 언어들이 얼마나 도전적인가? 우리는 표3에서 통계 수치를 보여준다. 정답이 단어 하나로 이루어진 기존의 많은 질의 응답 데이터셋들과 다르게 우리의 정답은 평균 7.5개 이상의 단어들로 구성되어있다. 이유(rationale)는 그보다 더 길고, 평균 16개의 단어들로 이루어져 있다.  

![]({{site.url}}/images/20210112/image4.png)
![]({{site.url}}/images/20210112/image5.png)

위의 그림은 각 QA 데이터셋의 빈도를 기준으로 오름차순으로 누적 확률분포(CDF)를 그린 것이다. 데이터셋의 응답들에서 10,000개씩 샘플링해서 그림을 그렸다.
두 예제에 대해서 토큰화(tokenization)와 표제어 추출(lemmatization), 불용어 처리(removal of stopwords) 한 후, 완전히 일치하는지를 고려한 것으로, 우리 데이터셋은 높은 다양성(diversity)를 보여주었다. 

A.2. Object covered
 평균적으로 봤을 때, 개략적으로(roughly) 두 객체들이 언급된다고 볼 수 있다. 대부분의 오브젝트들이 사람들이며 다른 오브젝트들도 COCO 오브젝트들로 흔하다. 오브젝트들은 흔히 ‘의자’, ‘타이’ 그리고 ‘컵’과 같은 흔히 관찰되는 것들이며 이러한 오브젝트들이 각 장면들에 따라 중요도가 다를 수 있다. 예를 들어 타이가 장면마다 등장하지만 질문과 응답에서는 차가 더 중요하게 언급될 수 있다. 어떤 오브젝트들은, 예를 들어 헤어드라이기나 스노우보드 같은, 거의 관찰되지 않을 수 있다. 
A.4. 추론 타입들 (Inference types)
데이터셋에 있는 상식과 인지(cognition) 수준에 대한 현상을 정확히 분류(categorize)한다는 것은 어려운 일이다. 하나의 접근법은 제시했던 그림2와 같이 질문을 추론 종류에 맞춰 분류해보는 것이다. 

![]({{site.url}}/images/20210112/image6.png)

표4. 각 질문의 종류를 결정하는 약간의(some) 규칙. 위와 같은 그룹들 중 하나의 단어가 포함되어 있는 어떤 질문에 대해서 결정한다. 예로 why가 있는 것은 추론 종류 explanation이라고 분류한다. 
우리는 추론 종류에 따라서 자동으로 분류를 하는 것이 이 태스크가 어려운 것을 필요로 한다는 것을 알 수 있었다. 왜냐하면 하나의 질문은 여러 추론들을 담을 수 있기 때문이다. 예로, ‘왜 사람1은 부끄러움을 느껴?”라는 질문에 대해서 추론하기 위해서는 사람1의 마음에 대해 추론할 수 있어야하고 뿐만 아니라 설명도 할 수 있어야 한다. 이러한 이유로, 우리는 우리의 태스크가 어려움이 내재되어 있다는 것을 주장한다.


E. 모델에 대한 자세한 설명 (Model details)
 이 섹션에서는 우리의 모델 R2C에 대한 구현에 대해 자세히 논의한다. 
	
BERT reprentations 
우리가 논문에 언급했듯이, 우리는 텍스트를 표현하는데 BERT를 사용했다. 우리는 우리의 모델과 BERT를 공정하게 비교하기를 원한다. 그래서 우리는 BERT-Base를 베이스라인으로써 이용했다. 
질의 q 그리고 응답 선택들 r(i) 우리는 단일 시퀀스 두 개를 합쳐서(merge) BERT에 준다(give). 하나의 예제는 다음과 같이 생겼다. [CLS] why is riley riding motorcycle while wearing a hospital gown? [SEP] she had to leave the hospital in a hurry. [SEP] 
위와 같은 예제들을 봤을 때, 우리는 사람 태그들을 중립적인 이름으로 대체하고 오브젝트 디텍션들을 클래스 이름들로 대체한다. 이는 BERT의 사전 학습 데이터들(BookCorpus)과 VCR의 도메인 쉬프트 (domain shift) 현상을 최소화 하기 위한 것이다. 
시퀀스의 각 토큰들은 BERT 안의 서로 다른 트랜스포머 유닛들에 대응된다. 우리는 BERT의 마지막 레이어를 문맥화 된 (contextualized) 표현들을 추출하는데 사용할 수 있다. 쿼리 및 응답들
우리는 freeze된 BERT의 representation들을 Transformer의 2번째부터 마지막 레이어들로부터 추출한다. 직관적으로 각 레이어들은 BERT의 사전학습 태스크들을 학습에 대해 이용되었기 때문에 표현을 사용하는데 이상하지 않다. 다음 문장을 맞추는 태스크 (마지막 레이어 L의 [CLS] 토큰이 이전 레이어 L-1 모두에 집중된다.) 뿐만 아니라, 마스크를 씌워 맞추는 태스크 (masked language modeling) 레이어 L에 있는 한 단어를 보는 유닛이 이전 레이어 L-1에서의 히든 스테이트 모두를 바라본다.) 버트 논문에 있는 실험에서는 이와 같은 작업들이 잘 되는 것을 제시할 뿐만 아니라 BERT를 파인튜닝을 end-to-end로 시키거나 활성화 함수를 가진 복수의 레이어들을 합치는(concatenate) 것에 대해 잘 되는 것을 보여준다. 
트레이드 오프로 그러나 BERT를 미리 계산해 놓는 것은 R2C 모델의 runtime을 감소시키고 우리가 더 강력한 vision representation을 배우는데 집중할 수 있도록 했다. 
Model Hyper parameters
R2C와 관련한 더 자세한 하이퍼파라미터 설명은 다음과 같다. 우리는 기본적인 세팅은 고정하고 가능하다면 유사한 configuration을 사용했다.

2176 차원의 이미지 맵 피처를 512로 프로젝션(projection). (ResNet50의 2048 차원과 클래스 임베딩에서의 128 차원 512 차원의 벡터 값)
Grounding LSTM은 단일 레이어의 양방향 LSTM으로 이루어져 있고, 1280 차원의 인풋 사이즈 형태 (BERT의 768 차원과 이미지 피처의 512 피처) 그리고 256 차원의 히든 스테이트 차원을 사용한다.
Reasoning LSTM의 경우 두 개의 양방향 LSTM을 사용하며 1536 차원의 인풋 사이즈 형태 (이미지 피처 512 사이즈, 256 차원의 각 방향 벡터, grounded된 쿼리와 grounded된 응답) 이것들은 256 히든 스테이트를 역시 사용한다.
모든 LSTM들은 우리는 히든 스테이트를 orthogonal initialization을 이용하여 초기화하며 LSTM에 rnn dropout을 사용하고, 그 확률은 p_drop=0.3 을 사용한다.
Restnet50의 백본은 사전학습 된 Imagenet을 사용했다. ResNet의 첫 세 블록은 파라미터를 고정시키고(frozen) 마지막 블록은 우리 모델을 위해서 파인 튜닝 과정을 거친다. RoiAlign은 그 이후에 적용한다. 우리는 이 표현들이 drift 될 것을 우려해서 모델에 보조 loss를 사용했다.
대부분, 이미지에 많은 오브젝트들이 있고 이것들이 쿼리나 응답들 집합에 참조되어 있지 않다.  


